---
title: LLMs Interview Note 1.3.4 LLM为什么采用Decoder only架构？
date: 2025-12-23 21:28:02
tags:
    - LLM
categories:
    - 学习笔记
---
***本文为 [LLMs Interview Note](https://github.com/wdndev/llm_interview_note) 的学习笔记。***

Transformer 模型包括 Encoder 和 Decoder 两部分。在设计上，Encoder 在提取一个 token 对应信息的时候可以看到整个序列中的所有 token，而 Decoder 只能看到其前面的所有 token。

现代的大模型主要有以下几种架构：

<!--more-->

- 以BERT为代表的 Encoder-only

- 以 T5 和 BART 为代表的 Encoder-Decoder

- 以 GPT 为代表的 Decoder-only

不过目前的大模型主要以 Decoder-only 架构偏多，原因主要有以下几条：

1. Encoder 中的低秩问题。Encoder 的 self-attention 不使用 casual mask，每个 token 都能看到其他所有 token，注意力矩阵容易退化成低秩的形式，进而弱化语言模型的表达能力。而 decoder 通常采用 casual attention（因果注意力），其 attention 矩阵是一个下三角矩阵，一定是满秩的，建模能力更强。

2. 预训练任务的难度。Decoder-only 模型在训练时由于只能看到前缀的所有 token，看不到下文，在这种情况下进行 next token prediction 的难度更高。可以感性地认为，当模型足够大，数据量足够高的时候，这种训练的方法能为 LLM 带来更高的能力上限。

3. 经验表明，zero-shot 和 in-context learning 场景下，decoder-only 模型的表现优于 encoder-decoder 模型。（[What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?](https://arxiv.org/abs/2204.05832)）一种解释是，在 Decoder-only 模型中，prompt 可以在整个文本生成的过程中持续地作用于模型，进而提升模型的 instruction-following 能力。 

4. 从计算性能上考虑，decoder-only 模型支持复用 KV Cache，对长上下文的场景更加友好；而 encoder-decoder 模型则难以实现这一点。
