---
title: LLMs Interview Note 1.2.1 分词
date: 2025-12-21 21:01:07
tags:
    - LLM
categories:
    - 学习笔记
---
***本文为 [LLMs Interview Note](https://github.com/wdndev/llm_interview_note) 的学习笔记。***

## 概念

分词是语言模型的基础。分词的质量直接关系着词性标注、句法分析等后续环节，也有语言模型的效果密切相关。英语以空格为天然的分词符，相比之下中文的分词则更加复杂，需要引入专门的分词算法。

<!--more-->

对中文进行分词是一项复杂的任务，其难点主要可以总结为三个方面：**分词标准**、**切分歧义**与**未登录词**。

- **分词标准**：不同方法的分词标准不同，例如对于姓名，有的方法认为应该分成姓+名两个词，而有的方法认为只需要分成一个词语

- **切分歧义**

  - 组合型歧义：一个词可以拆分为两个不同词的情况，如“将来”中的“将”和“来”都是单独的词，此时分词结果与上下文有关。

  - 交集型歧义：不同分词结果之间共用同一词的情况，如“南京市长江大桥”可以拆成“南京市/长江/大桥”或“南京/市长/江大桥”

  - 真歧义：句子本身具有歧义，无法准确分词

- **未登录词**：比如网络新词、专有名词、领域词汇等；未登录词对分词精度的影响极大，识别难度也很大。

## 中文分词算法

中文分词算法主要分为两类：**基于词典**以及**基于统计**的机器学习方法

基于词典的分词方法本质上是字符串匹配，包**括正向最大匹配法**（正向贪心选择token数最大的词汇）、**逆向最大匹配法**（逆向贪心处理）、**双向匹配分词**（正向与逆向各做一次，取词数最少的）、**全切分路径选择**（把分词问题视作DAG图上的最长路问题，并用动态规划求解）等

基于统计的分词方法主要包括基于隐马尔可夫模型（HMM）的方法、基于深度学习的方法等。这类方法将分词结果建模为一个长度与输入序列相同的字符串序列，只包含B/E/M三种字符（begin/end/middle），表示字在词语中所在的位置。其中，基于深度学习的方法可以将分词视作一个序列生成任务，并用有监督学习的方法训练。

jieba是一个集成了多种分词技术的开源中文分词框架，在效率和性能上都有不错的表现，被广泛应用。

