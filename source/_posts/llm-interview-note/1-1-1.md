---
layout: blog
title: LLMs Interview Note 1.1.1 语言模型
date: 2025/12/20 21:43:42
tags:
    - LLM
categories:
    - 学习笔记
---
***本文为 [LLMs Interview Note](https://github.com/wdndev/llm_interview_note) 的学习笔记。***

## 语言模型

语言模型，最初是在信息理论的背景下研究的。香农在信息论中引入了熵，用于衡量一段文本被压缩后的期望比特数；熵越小，代表文本的结构性越强，也即信息量越少，编码的长度也就越短。

现在所认为的语言模型，被定义为一个文本序列的概率分布。假设有一个token列表 $[v_1, ..., v_n]$，语言模型可以计算出其出现的概率 $p(v_1,\cdots,v_n)$。为了准确评估这一概率，语言模型需要具备强大的①语言能力与②世界知识。

为了高效地计算这一概率，现代的语言模型通常被建模为自回归的形式

$$p(a_1...,a_n)=p(a_1)\cdot p(a_2,...,a_n|a_1)=...$$

语言模型不仅可以用于计算，还可以被用作文本的生成任务，这一任务同样可以采用自回归的形式实现（next token prediction）。

对于一个自回归文本生成的任务，一个朴素的想法是直接采样出现概率最大的next token，也即

$$a_{n+1}=\argmax p(a_{n+1}|a_1,...,a_n)$$

这种方法也被称为 greedy decoding。

## 控制采样的三个参数：温度、top-k、top-p

为了让语言模型的输出更具随机性和多样性，可以考虑从next token的条件分布中进行采样。

我们可以通过 temperature, top-k, top-p 三个参数来控制采样的分布

temperature的作用是控制采样的随机性，具体来说，当 temperature=t 时，我们从 $p(x_i|x_{i...T})^{1/t}$ 中采样token。可以发现，温度这一参数具有以下性质：

- $t=0$ 时，等价于直接采样 argmax（即greedy decoding）

- $t=1$ 时为正常采样

- $t=\inf$ 时，相当于从整个词汇表上按照均匀分布采样

- 当 t 较高时，分布更加均匀，采样更具有随机性；反之，当 t 较小时，则采样更加稳定。

top-k 和 top-p 两个参数都是为了控制采样词表的大小。

首先看 top-k。在文本生成任务中，对于那些预测概率较小的垃圾token，依然有可能被选中并污染答案。为了避免这种情况，语言模型添加了 top-k 这一参数，只允许模型从出现概率最高的 k 个 token 中进行采样。此时还会对每个 token 被采样的概率作归一化，以保证所有 token 的总概率为 1。当 top-k=1 时，即等价于 greedy decoding。

top-p 的作用与 top-k 完全相同。但有一点不同的是，top-p 不用考虑词汇数量的多少，而是按照概率从大到小考虑当前选定 token 的概率之和，当这个概率和达到 top-p 时，我们便停止考虑剩下的 token，只从刚才取到的所有 token 里采样。

由于 top-k 和 top-p 都是同一个过程的两种不同策略，所以在语言模型中这两个参数是互斥的，只能指定两者其一。一般来说 top-p 的效果更好，因为 top-p 的自适应性更强，能根据上下文自动调整。比如在答案确定的简单位置，可能只对一两种token采样，来保证结果的正确性；但在开放性的问题下能对更多token进行采样，提升输出多样性。

## n-gram 模型

传统的语音识别和机器翻译任务使用了基于词的 n-gram 模型。

在 n-gram 模型中，对于词 $x_i$ 的预测仅依赖于后 n - 1 个词 $x_{i-n},...,x_{i-1}$，也即

$$p(x_i | x_{1:i-1})=p(x_i|x_{i-n:i-1})$$

这种模型在计算上非常高效，但推理效果较差，主要由于 n 的取值受到严重限制：

- n 的取值过小时，模型无法捕获长距离的依赖关系

- n 的取值过大时，又会导致在概率上无法得到一个好的统计，因为几乎所有 $p(x_{1:n})$ 都接近零

因此，n-gram 模型的应用被局限在语音识别等简单、上下文长度较短的任务上。

## 神经网络模型

2003年，基于神经网络的语言模型被提出。在这一模型里，词 i 的预测概率通过神经网络给出：

$$p(x _i|x_{1:i-1})=\dfrac{\text{NN}(x_{i-n:i})}{\text{NN}(x_{i-n:i-1})}$$

（这里NN表示神经网络参数）

Bengio等人提出，在相同数据量下，神经网络模型的表现优于 n-gram 模型。

虽然神经网络模型依然受到上下文长度 n 的限制，但神经网络的特性使其能够高效且准确地处理更长的上下文长度。现代的神经网络模型已经能够处理上百万 token 的上下文窗口。

神经网络模型的建模主要分为两种：基于RNN与基于Transformer的网络。相比RNN而言，Transformer能够利用GPU的并行特性，对长上下文进行高效处理。

