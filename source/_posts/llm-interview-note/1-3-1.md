---
title: LLMs Interview Note 1.3.1 Word2Vec
date: 2025-12-23 19:50:04
tags:
    - LLM
categories:
    - 学习笔记
---
***本文为 [LLMs Interview Note](https://github.com/wdndev/llm_interview_note) 的学习笔记。***

## 概念

Word2Vec的思路是把单词转化为向量表示，进而便于计算机进行处理。

传统方法包括采用one-hot对单词进行表示，但这一方法存在缺陷：一是单词表可能很大，甚至达到百万级别，将词汇编码成如此大的向量对后续过程的计算造成了很大的挑战；二是one-hot编码的向量都是彼此正交的，无法体现词汇之间的相关性关系。

Word2Vec使用distributed representation解决了这些问题，其思想是把每个one-hot向量都转化为一个较短的、彼此不正交的向量表示。

## 训练方法

Word2Vec的训练方法分为两种：CBOW（Continuous Bag-of-Words，词袋模型）和Skip-gram。

CBOW的做法类似完形填空：给定一个token周围的若干token，让神经网络去预测中心token；Skip-gram的做法则与其相反：要求模型基于给定的中心token来预测周围的语境。

**一般来说，CBOW适合数据集较小的情况，而Skip-gram则在大型数据集上表现更好。**

个人理解：

因为CBOW是用周围很多token去预测一个中心token，这样的输入信号更加平滑、方差更小，因此能让模型学习到稳定的分布，相对不容易过拟合。

而Skip-gram的做法更加激进，用少量token去预测周围很多token，这种做法能让模型学习到对中心token更加细粒度的表示。此外，这种方法对低频词更加友好，因为每个词都有机会作为中间token输入被更新权重。

在小数据环境下，Skip-gram这种激进的做法，会让更新的梯度方差过大，容易让模型过度更新参数，进而学习到不稳定的表示，但在大数据集下这种趋势会被庞大的数据量所平滑。

### 训练过程

![image.webp](image.webp)

在训练时，CBOW有一个超参数 $c$，用来控制输入的token数量。在CBOW中可训练的参数有两种：token embedding矩阵 $W^{n\times d}$，以及映射矩阵 $W_r^{d\times n}$。首先，输入的 token 会被 embedding 矩阵映射成对应的向量并相加作为隐藏层向量，即

$$h=\sum_{i=1}^c W(a_i),~h \in \R^{d}$$

然后隐藏层向量会与映射矩阵相乘，经过激活函数，得到对每个 token 的概率预测：

$$p=f(h \cdot W_r),~p \in \R^{n}$$

这里的损失通常采用交叉熵损失函数来计算。

在Skip-gram的训练中需要注意的是，一次传播过程中，只有一个 output token，模型通过对海量数据的采样，可以学习到一个具有统计学意义的分布；这也决定了 skip-gram 模型的训练过程是相对不稳定的，因为同一个 input 可能对应不同的 groundtruth。

Skip-gram 训练中有两个参数：skip_window，用来控制采样的窗口大小，以及 num_skip 参数，用来控制对同一个 input token 采样周围 token 的次数，也就是对同一个中心token要生成多少个训练数据。Skip-gram 的训练方法与 CBOW 类似，在此不再赘述。

不过，在实际应用中，这两种训练方法的计算开销是很大的。为了减少计算，人们开发了Hierarchical Softmax 和 Negative Sampling 两种方法。

## Hierarchical Softmax

Hierarchical Softmax（层次softmax） 不再为词表中的每个 token 直接维护一个输出向量，而是把所有 token 编码成哈夫曼树上的叶子节点，并对每个非叶子节点引入了一个由参数 $\theta$ 控制的二分类决策函数。模型沿着从根节点到叶子节点的路径，进行一系列二分类判断来计算对应 token 的概率。这种做法可以把原本线性的时间复杂度降低到对数级别。

![image.webp](image%201.webp)


## Negative Sampling

Negative Sampling（负采样）基于这样一个观察：在每次训练时，即使当前的样本只和几个token相关，神经网络也需要去更新所有 token 的参数，这样会产生非常多的无效计算。为了缓解这一问题，人们引入了负采样，把参数更新的范围只局限于正样本，以及随机选出的 k 个负样本。

具体来说，Skip-gram 原本的优化目标是：

$$\max \log p(c~|~w) = \log \dfrac{\exp(\mathbf{u}_c^T\mathbf{v}_w)}{\sum_{i \in V}\exp(\mathbf{u}_i^T\mathbf{v}_w)}$$

可以看出，这一优化目标涉及到对所有 token 对应 u 参数矩阵的计算和更新。

负采样把这一目标变为

$$\mathcal{L}=\log \sigma(\mathbf{u}_c^T\mathbf{v}_w)+\sum_{i=1}^k \mathbb{E}_{c_i\sim P_n} [\log \sigma(-\mathbf{u}_{v_i}^T\mathbf{v}_w)]$$

这里 $P_n$ 是人为指定的负采样分布，$k$ 是负样本数量。

在 Word2Vec 中推荐的负采样分布是 temperature=3/4 的词频分布，即

$$P_n(w) \propto f(w)^{3/4}$$

这里 $f(w)$ 为词频。



