---
title: LLMs Interview Note 2.1.5 预训练模型效果的影响因素
date: 2026/01/02 11:43:29
tags:
    - LLM
categories:
    - 学习笔记
---

***本文为 [LLMs Interview Note](https://github.com/wdndev/llm_interview_note) 的学习笔记。***

## Scaling Law (Training time)

预训练模型的性能强相关于三个因素：模型参数量N、训练数据量D、训练总计算量C。在其他因素不成为瓶颈，且训练未饱和的情况下，模型性能与N、D、C都呈现出近似的幂律关系，即

$$L\propto X^{-\alpha}$$

<!--more-->

其中 $L$ 代表模型性能，$X\in \{N,D,C\}$

在给定计算量预算下，模型参数量与训练数据量应同比提升，即 $D \propto N$

## 预训练中数据重复对性能的影响

在预训练过程中，如果同一批token被模型反复看到（如采用多个epoch），模型的整体性能反而可能会下降，此时模型会对训练数据产生过拟合，而不是学到更有泛化性的表示。

在采用了多个epoch的情况下，扩大数据集规模能在一定程度上抵消它对模型性能的影响，因为这种做法增强了模型的泛化性；如果不扩大规模，而是选择提升数据集质量，则无法挽救这种过拟合。

![大规模数据集可以抵消重复训练的影响](image.webp)

<div class="img-caption">大规模数据集可以抵消重复训练的影响</div>

这种过拟合现象同时存在于小规模和大规模的模型中，说明不是模型表示能力不足导致的，而是一种训练机制上的普遍问题。

![这一现象同时存在于不同规模的模型中](image%201.webp)
<div class="img-caption">这一现象同时存在于不同规模的模型中</div>

越大规模的模型对重复训练的抵抗能力越强，但依然无法根本解决问题；但相同参数规模不同计算量的模型（MoE、ParamShare等）在性能上趋势相同，这启示我们可以使用较小的计算量来预估趋势。

![大模型能抵抗重复训练的影响；扩大计算量则没有效果](image%202.webp)
<div class="img-caption">大模型能抵抗重复训练的影响；扩大计算量则没有效果</div>

采用多样化的训练目标，而无法抵消这种性能下降，反而更加容易受到这种性能损失。

### 正则化可以缓解这种影响吗

Dropout虽然是一种在LLM中常常被忽视的技术，却可以有效地缓解这种现象。

虽然相对较慢，但可以抵消重复数据带来的影响。而且相比于从一开始直接使用Dropout，在后续迭代中逐渐引入Dropout的效果更好。

![Dropout可以缓解这种过拟合](image%203.webp)
<div class="img-caption">Dropout可以缓解这种过拟合</div>

需要注意的是，对于不同规模的模型，Dropout的效果并不完全一致；在规模较大的模型中，效果可能较差

## SFT的数据量

- 在SFT中，不需要大量的微调数据

    - 采用少量高质量、多样化的数据，也可以训练出效果优秀的SFT模型。



