<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="false" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P8MWPQK9PH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-P8MWPQK9PH');
</script>
<!-- End Google Analytics -->

  
  
<!-- Gaug.es Analytics -->
<script>
  var _gauges = _gauges || [];
  (function() {
    var t   = document.createElement('script');
    t.async = true;
    t.id    = 'gauges-tracker';
    t.setAttribute('data-site-id', '6946e8d63391706f9651c69b');
    t.setAttribute('data-track-path', 'https://track.gaug.es/track.gif');
    t.src = 'https://d36ee2fcip1434.cloudfront.net/track.js';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(t, s);
  })();
</script>
<!-- End Gaug.es Analytics -->


  
  <title>Glerium&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css" />
  <meta name="google-site-verification" content="vQvmfVLpkyuFa1L0s5rTR6LDX0OS5FL2lPE0EykmpuY" />

  <meta property="og:type" content="website">
<meta property="og:title" content="Glerium&#39;s Blog">
<meta property="og:url" content="https://glerium.github.io/index.html">
<meta property="og:site_name" content="Glerium&#39;s Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="glerium">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Glerium's Blog" type="application/atom+xml">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
<link rel="stylesheet" href="/css/style.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 8.1.1"></head>

<body>
  
   
  <div id="main-grid" class="  ">
    <div id="nav" class=" is_home "  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>Glerium's Blog </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
      <a id="nav-rss-link" class="nav-icon mobile-hide" href="/atom.xml" title="RSS 订阅">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M198-120q-25.846 0-44.23-18.384-18.384-18.385-18.384-44.23 0-25.846 18.384-44.23 18.384-18.385 44.23-18.385 25.846 0 44.23 18.385 18.384 18.384 18.384 44.23 0 25.845-18.384 44.23Q223.846-120 198-120Zm538.385 0q-18.846 0-32.923-13.769-14.076-13.769-15.922-33.23-8.692-100.616-51.077-188.654-42.385-88.039-109.885-155.539-67.5-67.501-155.539-109.885Q283-663.462 182.385-672.154q-19.461-1.846-33.23-16.23-13.769-14.385-13.769-33.846t14.076-32.922q14.077-13.461 32.923-12.23 120.076 8.692 226.038 58.768 105.961 50.077 185.73 129.846 79.769 79.769 129.846 185.731 50.077 105.961 58.769 226.038 1.231 18.846-12.538 32.922Q756.461-120 736.385-120Zm-252 0q-18.231 0-32.423-13.461t-18.653-33.538Q418.155-264.23 348.886-333.5q-69.27-69.27-166.501-84.423-20.077-4.462-33.538-18.961-13.461-14.5-13.461-33.346 0-19.076 13.884-33.23 13.884-14.153 33.115-10.922 136.769 15.384 234.384 112.999 97.615 97.615 112.999 234.384 3.231 19.23-10.538 33.115Q505.461-120 484.385-120Z"/></svg>
      </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">Home</a>
    
      <a class="nav-dropdown-link" href="/archives">Archives</a>
    
      <a class="nav-dropdown-link" href="/about">About</a>
    
    
      <a class="nav-dropdown-link" href="/atom.xml" title="RSS 订阅">RSS</a>
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/avatar.png></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">glerium </div>
      <div class="dot"></div>
      <div class="subtitle"> </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com/glerium" title="GitHub"><i class="fa-brands fa-github"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      


  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">分类</h3>
      <div class="category-box">
            <a class="category-link" href="/categories/%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B/">
                算法竞赛
                <div class="category-count">4</div>
            </a>
        
            <a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                学习笔记
                <div class="category-count">4</div>
            </a>
        
            <a class="category-link" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">
                课程笔记
                <div class="category-count">5</div>
            </a>
        
            <a class="category-link" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">
                论文笔记
                <div class="category-count">4</div>
            </a>
        </div>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">标签</h3>
      <ul class="widget-tag-list" itemprop="keywords"><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/DNA%E7%94%B2%E5%9F%BA%E5%8C%96/" rel="tag">DNA甲基化</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" rel="tag">动态规划</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag">数学</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/" rel="tag">数理统计</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>
    </div>
  </div>


    
  </div>
</sidebar>
    </div>
    <div id="content-body">
      
   


<article id="post-llm-interview-note/1-3-4" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 itemprop="name">
        <a class="p-name article-title" href="/2025/12/23/llm-interview-note/1-3-4/">LLMs Interview Note 1.3.4 LLM为什么采用Decoder only架构？</a>
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-12-23T13:28:02.000Z" itemprop="datePublished">2025-12-23</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            882 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <div class="truncate-text">
            
              本文为 LLMs Interview Note 的学习笔记。
Transformer 模型包括 Encoder 和 Decoder 两部分。在设计上，Encoder 在提取一个 token 对应信息的时候可以看到整个序列中的所有 token，而 Decoder 只能看到其前面的所有 token。
现代的大模型主要有以下几种架构：


以BERT为代表的 Encoder-only


以 T5 和 BART 为代表的 Encoder-Decoder


以 GPT 为代表的 Decoder-only


不过目前的大模型主要以 Decoder-only 架构偏多，原因主要有以下几条：


Encoder-Decoder 中的低秩问题。Encoder 的 self-attention 不使用 casual mask，每个 token 都能看到其他所有 token，注意力矩阵容易退化成低秩的形式，进而弱化语言模型的表达能力。而 decoder 通常采用 casual attention（因果注意力），其 attention 矩阵是一个下三角矩阵，一定是满秩的，建模能力更强。


预训练...
            
          </div>
        
      </div>

         
    </div>
    
     
    <a class="right-panel  non-pic  "
      
      href="/2025/12/23/llm-interview-note/1-3-4/" 
    >
       
      <i class="fa-solid fa-angle-right non-pic"></i>
       
    </a>
     
  </div>
  
</article>






   


<article id="post-llm-interview-note/1-3-1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 itemprop="name">
        <a class="p-name article-title" href="/2025/12/23/llm-interview-note/1-3-1/">LLMs Interview Note 1.3.1 Word2Vec</a>
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-12-23T11:50:04.000Z" itemprop="datePublished">2025-12-23</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            2.4k 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <div class="truncate-text">
            
              本文为 LLMs Interview Note 的学习笔记。
概念
Word2Vec的思路是把单词转化为向量表示，进而便于计算机进行处理。
传统方法包括采用one-hot对单词进行表示，但这一方法存在缺陷：一是单词表可能很大，甚至达到百万级别，将词汇编码成如此大的向量对后续过程的计算造成了很大的挑战；二是one-hot编码的向量都是彼此正交的，无法体现词汇之间的相关性关系。
Word2Vec使用distributed representation解决了这些问题，其思想是把每个one-hot向量都转化为一个较短的、彼此不正交的向量表示。
训练方法
Word2Vec的训练方法分为两种：CBOW（Continuous Bag-of-Words，词袋模型）和Skip-gram。
CBOW的做法类似完形填空：给定一个token周围的若干token，让神经网络去预测中心token；Skip-gram的做法则与其相反：要求模型基于给定的中心token来预测周围的语境。
一般来说，CBOW适合数据集较小的情况，而Skip-gram则在大型数据集上表现更好。
个人理解：
因为CBOW是用周围很多t...
            
          </div>
        
      </div>

         
    </div>
    
     
    <a class="right-panel  non-pic  "
      
      href="/2025/12/23/llm-interview-note/1-3-1/" 
    >
       
      <i class="fa-solid fa-angle-right non-pic"></i>
       
    </a>
     
  </div>
  
</article>






   


<article id="post-llm-interview-note/1-2-1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 itemprop="name">
        <a class="p-name article-title" href="/2025/12/21/llm-interview-note/1-2-1/">LLMs Interview Note 1.2.1 分词</a>
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-12-21T13:01:07.000Z" itemprop="datePublished">2025-12-21</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            774 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <div class="truncate-text">
            
              本文为 LLMs Interview Note 的学习笔记。
概念
分词是语言模型的基础。分词的质量直接关系着词性标注、句法分析等后续环节，也有语言模型的效果密切相关。英语以空格为天然的分词符，相比之下中文的分词则更加复杂，需要引入专门的分词算法。
对中文进行分词是一项复杂的任务，其难点主要可以总结为三个方面：分词标准、切分歧义与未登录词。


分词标准：不同方法的分词标准不同，例如对于姓名，有的方法认为应该分成姓+名两个词，而有的方法认为只需要分成一个词语


切分歧义


组合型歧义：一个词可以拆分为两个不同词的情况，如“将来”中的“将”和“来”都是单独的词，此时分词结果与上下文有关。


交集型歧义：不同分词结果之间共用同一词的情况，如“南京市长江大桥”可以拆成“南京市/长江/大桥”或“南京/市长/江大桥”


真歧义：句子本身具有歧义，无法准确分词




未登录词：比如网络新词、专有名词、领域词汇等；未登录词对分词精度的影响极大，识别难度也很大。


中文分词算法
中文分词算法主要分为两类：基于词典以及基于统计的机器学习方法
基于词典的分词方法本质上是字符串匹配，包括正...
            
          </div>
        
      </div>

         
    </div>
    
     
    <a class="right-panel  non-pic  "
      
      href="/2025/12/21/llm-interview-note/1-2-1/" 
    >
       
      <i class="fa-solid fa-angle-right non-pic"></i>
       
    </a>
     
  </div>
  
</article>






   


<article id="blog-llm-interview-note/1-1-1" class="h-entry article article-type-blog" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 itemprop="name">
        <a class="p-name article-title" href="/2025/12/20/llm-interview-note/1-1-1/">LLMs Interview Note 1.1.1 语言模型</a>
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-12-20T13:43:42.000Z" itemprop="datePublished">2025-12-20</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            2.3k 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <div class="truncate-text">
            
              本文为 LLMs Interview Note 的学习笔记。
语言模型
语言模型，最初是在信息理论的背景下研究的。香农在信息论中引入了熵，用于衡量一段文本被压缩后的期望比特数；熵越小，代表文本的结构性越强，也即信息量越少，编码的长度也就越短。
现在所认为的语言模型，被定义为一个文本序列的概率分布。假设有一个token列表 [v1,...,vn][v_1, ..., v_n][v1​,...,vn​]，语言模型可以计算出其出现的概率 p(v1,⋯ ,vn)p(v_1,\cdots,v_n)p(v1​,⋯,vn​)。为了准确评估这一概率，语言模型需要具备强大的①语言能力与②世界知识。
为了高效地计算这一概率，现代的语言模型通常被建模为自回归的形式
p(a1...,an)=p(a1)⋅p(a2,...,an∣a1)=...p(a_1...,a_n)=p(a_1)\cdot p(a_2,...,a_n|a_1)=...
p(a1​...,an​)=p(a1​)⋅p(a2​,...,an​∣a1​)=...
语言模型不仅可以用于计算，还可以被用作文本的生成任务，这一任务同样可以采用自回归的...
            
          </div>
        
      </div>

         
    </div>
    
     
    <a class="right-panel  non-pic  "
      
      href="/2025/12/20/llm-interview-note/1-1-1/" 
    >
       
      <i class="fa-solid fa-angle-right non-pic"></i>
       
    </a>
     
  </div>
  
</article>






   


<article id="post-paper-notes/multimodal-token-compression" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 itemprop="name">
        <a class="p-name article-title" href="/2025/12/14/paper-notes/multimodal-token-compression/">[论文笔记] When Tokens Talk Too Much: A Survey of  Multimodal Long-Context Token Compression  across Images, Videos, and Audios</a>
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-12-14T14:08:30.000Z" itemprop="datePublished">2025-12-14</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            606 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <div class="truncate-text">
            
              Token压缩的动机


self-attention二次方复杂度带来的计算挑战 → 加速推理和减少内存消耗

在MLLM中更加明显，visual / audio数据的token数量比纯文本高几个数量级




为什么有效
多模态数据中有大量信息冗余（图片：相邻像素；视频：周围的重复帧；音频：相近时间与相近频谱之间的冗余）
例如，高分辨率图像包含很强的局部相关性，而视频流在帧之间具有广泛的时空冗余，音频信号通常包含扩展的静默段或平稳噪声。
优势


加速推理，减少内存消耗


作为后训练方法，无需重新训练模型


现有方法分类


基于模态分类：image-centric, video-centric, audio-centric


基于机制分类：


transformation-based：快速，但压缩效果通常一般（~25%）


pixel-shuffle (training-free)


pooling / interpolation (training-free)


convolution




similarity-based：较为灵活，但可能丢失信息


a...
            
          </div>
        
      </div>

         
    </div>
    
     
    <a class="right-panel  non-pic  "
      
      href="/2025/12/14/paper-notes/multimodal-token-compression/" 
    >
       
      <i class="fa-solid fa-angle-right non-pic"></i>
       
    </a>
     
  </div>
  
</article>






   


<article id="blog-paper-notes/gpt1" class="h-entry article article-type-blog" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 itemprop="name">
        <a class="p-name article-title" href="/2025/07/18/paper-notes/gpt1/">[论文笔记] Improving Language Understanding by Generative Pre-Training (GPT-1)</a>
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-07-17T16:00:00.000Z" itemprop="datePublished">2025-07-18</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            548 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <div class="truncate-text">
            
              研究背景


现实生活中无标签的文本数据远多于带标签的数据


现有的许多模型依赖于大量任务特定的、带标签的数据进行训练，十分费时费力


无监督的方式需要考虑两大问题：


在pretrain时采用什么任务对提取知识最有效果（解决方法：生成式任务）


如何将无监督中学习的知识应用到下游任务上（解决方法：minimal change to structure during finetuning）






亮点


对于不同任务而言，GPT-1的模型架构是统一的，微调时对架构的更改很少


在9 of 12个任务上取得了SOTA


证明了在多个任务上，基于广泛无标签语料的生成式预训练可以提升模型效果


方法


模型架构：12层的Transformer decoder-only架构


预训练阶段：h = transformer(x), y=We(h)y = W_e(h)y=We​(h)


微调阶段：h = transformer(x), y=Wy(h)y = W_y(h)y=Wy​(h)


这样在微调阶段，只需要训练 WyW_yWy​ 矩阵即可




预训练阶段
...
            
          </div>
        
      </div>

         
    </div>
    
     
    <a class="right-panel  non-pic  "
      
      href="/2025/07/18/paper-notes/gpt1/" 
    >
       
      <i class="fa-solid fa-angle-right non-pic"></i>
       
    </a>
     
  </div>
  
</article>






   


<article id="blog-paper-notes/gpt2" class="h-entry article article-type-blog" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 itemprop="name">
        <a class="p-name article-title" href="/2025/07/17/paper-notes/gpt2/">[论文笔记] Language Models are Unsupervised Multitask Learners (GPT-2)</a>
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-07-16T16:00:00.000Z" itemprop="datePublished">2025-07-17</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            597 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <div class="truncate-text">
            
              背景


现有的模型大多基于较小的语料，而且是针对特定任务的


从meta-learning的角度来看，multi-task learning需要相当多的 (task, dataset) 组合作为train sample，这是不现实的


创新点


提出了一个更大的LM架构，包含三种大小：GPT-1、BERT、GPT-2


提出了一个LM语料库，从Reddit的优质贴外链汇总得到

为了避免train set和test set重叠，删除了Wikipedia的网页



仅在大型语料库采用pretrain的情况下，发现模型能天然地适配于各种任务，而无需微调

一个无监督的模型在许多任务上取得了SOTA效果



证明了基于极大似然估计的语言模型能够拟合现实任务


缺点

在部分任务上，GPT-2的定量性能依然较差，不具备现实应用的能力

架构


大致与GPT-1相同，但有几点改进


Layer normalization从post-norm改为了pre-norm，这样可以提高训练的稳定性


每一层的initialization参数都进行了缩放，变为先前的 1/sqrt...
            
          </div>
        
      </div>

         
    </div>
    
     
    <a class="right-panel  non-pic  "
      
      href="/2025/07/17/paper-notes/gpt2/" 
    >
       
      <i class="fa-solid fa-angle-right non-pic"></i>
       
    </a>
     
  </div>
  
</article>






   


<article id="post-math/operational-research" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 itemprop="name">
        <a class="p-name article-title" href="/2024/12/20/math/operational-research/">运筹学课程笔记</a>
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2024-12-20T12:58:24.000Z" itemprop="datePublished">2024-12-20</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            1.5k 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag">数学</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <div class="truncate-text">
            
              

规划论


线性规划


不等式转化为等式约束：添加松弛变量

如果松弛变量为零，表明资源全部用完，该资源是匮乏的；如果松弛变量为正，表明资源尚有余存，该资源是充裕的



无限制变量：拆成两个 y=y−−y+y=y^--y^+y=y−−y+，单纯形法中，二者不可能同时取正


单纯形法：


首先构造初始解，xi=0x_i=0xi​=0


选进基变量：系数最大；离基变量：z/a最小


枢轴行：1) 在基列中以进基变量替换离基变量 2) 新的枢轴行=当前枢轴行÷枢轴元素
其他所有行，包括Z行：新的行=当前行-当前枢轴列的系数×新的枢轴列




阶段一试图求一个初始基本可行解，找到一个解以后，调用阶段二求解原问题

一阶段目标值为0表示解存在



单纯形法的特殊情况：


退化：约束多余，至少有一个基变量在下一次迭代中为零，并称新的解退化。遇到退化时不可以停止计算，因为可能出现暂时退化。


可选择最优解：目标函数可以在多于一个解点处相同的最优解


无界解：解空间中至少又一个变量是无界的


不可行解：具有不相容约束的线性规划模型没有可行解。仅当模型有可行空间时，人工...
            
          </div>
        
      </div>

         
    </div>
    
     
    <a class="right-panel  non-pic  "
      
      href="/2024/12/20/math/operational-research/" 
    >
       
      <i class="fa-solid fa-angle-right non-pic"></i>
       
    </a>
     
  </div>
  
</article>






   


<article id="post-math/machine-learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 itemprop="name">
        <a class="p-name article-title" href="/2024/06/18/math/machine-learning/">机器学习课程笔记</a>
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2024-06-18T06:07:00.000Z" itemprop="datePublished">2024-06-18</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            9.7k 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag">数学</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <div class="truncate-text">
            
              

例题集锦


2-4



3-7



4-7



4-13




5-2



6-6



11-8










12-5

Gini指数算决策树，答案同上



13-4




13-15




16-30（第一张是random policy，第二张是optimal policy）




21-7





numpy函数整理


np.arange(a, b, step) 为[a,b)左开右闭区间


np.random.normal(mu, sigma, shape)：生成正态分布的随机数


np.linalg.eig(C)：求特征向量、特征值


np.diag()：对角矩阵


np.linalg.inv(C)：求逆矩阵


np.linalg.pinv(C)：矩阵的伪逆


np.linalg.det(C)：求行列式


np.where(condition)：返回condition为真的index list，元素长度等同于condition维度

np.where(condition, x, y)：当condition在对应位置为真时返...
            
          </div>
        
      </div>

         
    </div>
    
     
    <a class="right-panel  non-pic  "
      
      href="/2024/06/18/math/machine-learning/" 
    >
       
      <i class="fa-solid fa-angle-right non-pic"></i>
       
    </a>
     
  </div>
  
</article>






   


<article id="post-math/real-analysis" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 itemprop="name">
        <a class="p-name article-title" href="/2024/06/13/math/real-analysis/">实变函数课程笔记</a>
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2024-06-13T14:33:00.000Z" itemprop="datePublished">2024-06-13</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            6.9k 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag">数学</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <div class="truncate-text">
            
              第一章


任何无限集必有一个可列子集

对无限集而言，一定存在某个它的真子集，使得该子集与原集合对等



[0,1] 是不可列的（用闭区间套定理证明）


任意个开集的并是开集

任意个闭集的交是闭集



有限个开集的交是开集

有限个闭集的并是闭集



聚点：E 为点集，a 的任一邻域中都含有 E 中异于 a 的点（据此可以构造一个含于 E 的点列趋近于 a）


聚点 a 的任意邻域中都有无穷多个异于 a 的点


若集合中的任意一点都为聚点，则该集合为闭集




开集定义略，闭集为开集关于R的补集


导集：E 中一切聚点构成的集合 E’

任何集合的导集一定为闭集（通过取补集证明）



孤立点：集合中不是聚点的点 E \ E’


闭包：集合与其导集的并 E‾=E∪E′\overline{E}=E \cup E{&#x27;}E=E∪E′

闭集的闭包与其本身相等（由闭集的导集含于原集合这一点可直接推出）



完全集：E = E’

若闭集中没有孤立点，则其为完全集



f(x)f(x)f(x) 连续   ⟺  \iff⟺f(x)f(x)f(x) 关于任意...
            
          </div>
        
      </div>

         
    </div>
    
     
    <a class="right-panel  non-pic  "
      
      href="/2024/06/13/math/real-analysis/" 
    >
       
      <i class="fa-solid fa-angle-right non-pic"></i>
       
    </a>
     
  </div>
  
</article>







  <nav id="page-nav">
    <!-- 在第二页时 page.prev_link 不知为何无法返回正确的链接 -->
    <a id="prev-btn" class="page-nav-btn disabled" href=" /  ">
      <i class="fa-solid fa-angle-left"></i>
    </a>
    <div id="num-bar">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a>  
    </div>
    <a id="next-btn" class="page-nav-btn " href="/page/2/">
      <i class="fa-solid fa-angle-right"></i>
    </a>
  </nav>


    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2025 glerium<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
</body>
</html>
